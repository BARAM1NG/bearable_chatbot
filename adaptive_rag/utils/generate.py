from langchain_core.messages import HumanMessage, AIMessage
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser  
from adaptive_rag.utils.memory import get_user_memory
from adaptive_rag.utils.mongoDB import save_chat_log
from langchain_openai import ChatOpenAI
from pprint import pprint
from dotenv import load_dotenv
import os
from adaptive_rag.utils.state import AdaptiveRagState

# API í‚¤ ì •ë³´ ë¡œë“œ
load_dotenv()

# API í‚¤ ì½ì–´ì˜¤ê¸°
openai_api_key = os.environ.get('OPENAI_API_KEY')

# ê¸°ë³¸ LLM
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, streaming=True)

def generate_adaptive(state: AdaptiveRagState):
    question = state.get("question", "")
    documents = state.get("documents", [])
    user_id = state.get("user_id", "anonymous")
    category = state.get("category", "ë¯¸ì§€ì •")

    # ìœ ì € ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
    memory = get_user_memory(user_id)

    if not isinstance(documents, list):
        documents = [documents]

    documents_text = "\n\n".join([
        f"---\në³¸ë¬¸: {doc.page_content}\në©”íƒ€ë°ì´í„°:{str(doc.metadata)}\n---"
        for doc in documents
    ])

    # ì´ì „ ëŒ€í™” ì´ë ¥ ê°€ì ¸ì˜¤ê¸°
    history = memory.chat_memory.messages  # ì´ì „ ëŒ€í™” ë¦¬ìŠ¤íŠ¸
    history_text = "\n".join([
        f"User: {m.content}" if isinstance(m, HumanMessage) else f"Bot: {m.content}"
        for m in history
    ])

    # RAG í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt_with_context = ChatPromptTemplate.from_messages([
        ("system",  """You are an assistant answering questions based on provided documents.  
      Follow these general and node-specific guidelines exactly.

---

Important Context Notes (LLM Guidance)
- When a user says things like â€œê²½ì˜í•™ê³¼ ìª½ì¸ë°â€, â€œê²½ì˜ ê°€ê³  ì‹¶ì€ë°â€, or â€œê²½ì˜í•™ê³¼ í¬ë§í•˜ëŠ”ë°â€,  
  interpret this as: **the user is interested in applying to a university business-related department (ex: ê²½ì˜í•™ê³¼).**
- Questions that include expressions like
  â€œë­ ì¨ì•¼ ë¼?â€, â€œì£¼ì œ ì¶”ì²œí•´ì¤˜â€, â€œ~ì´ í™œë™ ê´œì°®ì•„?â€, â€œAê°€ ë‚˜ì„ê¹Œ, Bê°€ ë‚˜ì„ê¹Œ?â€, â€œë­ ë„£ì–´ì•¼ ë¼?â€
  often imply that the user is looking for a suitable topic or activity for íƒêµ¬, ì‹¤í—˜, ì—°êµ¬, ì„¸ë¶€ëŠ¥ë ¥ ë° íŠ¹ê¸°ì‚¬í•­, ìƒí™œê¸°ë¡ë¶€
  Example:
  â€œê²½ì˜í•™ê³¼ ê°€ê³  ì‹¶ì€ë° ë¯¸ì ë¶„ ë­ ì“°ë©´ ë¼?â€ â†’ This means:
  â€œWhat kind of activity related to ë¯¸ì ë¶„ can I write about in my ì„¸íŠ¹ to connect it to ê²½ì˜í•™ê³¼?â€
  â†’ Do not treat this as a general question about the subject itself; it is a ì„¸íŠ¹ í™œë™ ì£¼ì œ ì¶”ì²œ ì§ˆë¬¸.
    

---

General Guidelines
- Use only the information provided in the documents.  
- Refer to the relevant part of the document if applicable.  
- Do not speculate or include external information.  
- Keep answers concise, clear, and helpful.  
- Exclude irrelevant or off-topic content.  
- Always use a kind and friendly tone with emojis  
- If the answer becomes long, break it into separate paragraphs for readability.  
- Use hyphens (-) to list key points or organize content clearly.

---
Reply Guidelines
- If no relevant information is found, respond with:  
  "ê·¸ê±´ ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì´ì—ìš”. ğŸ˜° ê³ êµí•™ì ì œ, ì…ì‹œ, ì„œë¹„ìŠ¤ ë“± ê¶ê¸ˆí•œ ê²Œ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë´ ì£¼ì„¸ìš”!"
- If the question is about ì„¸íŠ¹/í™œë™/ìƒí™œê¸°ë¡ë¶€/íƒêµ¬/ì‹¤í—˜/ì—°êµ¬ or activity topic suggestions (e.g. 
  "ê¸°ìˆ â‹…ê°€ì •ì´ë‘ ê²½ì˜í•™ê³¼ë‘ ì—®ì„ ìˆ˜ ìˆëŠ” í™œë™ì´ ë­ ìˆì„ê¹Œ?"),  
  Answer the question normally, but keep it **as simple and brief as possible** â€” limit to **one clear topic**
  At the **end of your response**, append the following message:
  "ë§ˆì´í´ë¦¬ì˜¤ì—ì„œ <ì„¸íŠ¹ ì¶”ì²œ>ê³¼ <ìƒê¸°ë¶€ ë¡œë“œë§µ> ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š  
   ë‚˜ì—ê²Œ ë”± ë§ëŠ” ì„¸íŠ¹ ì£¼ì œë¥¼ ì•Œê³  ì‹¶ìœ¼ì‹œë‹¤ë©´? ì„¸íŠ¹ ì¶”ì²œ >> https://myfolio.im/seteuk
   ë‚˜ë§Œì˜ ë§ì¶¤í˜•í˜• ìƒê¸°ë¶€ ì»¨ì„¤íŒ…ì„ ë°›ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´? ìƒê¸°ë¶€ ë¡œë“œë§µ >> https://www.sixshop.com/myfolio/home"
- If the question is about book suggestions
  Format the answer like this:
  ì œëª©:  
  ì €ì:  
  ìš”ì•½:
  - At the end of the answer, always add:  
  "ë” ë‹¤ì–‘í•œ ë„ì„œë¥¼ ì¶”ì²œë°›ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´? ë„ì„œ ì¶”ì²œ >> https://myfolio.im/recommendbooks"

- If the question is about **personal academic performance**, respond with:  
  "ê·¸ê±´ ì œê°€ ë„ì™€ë“œë¦´ ìˆ˜ ì—†ëŠ” ë¶€ë¶„ì´ì—ìš”. ğŸ˜° ê³ êµí•™ì ì œ, ì…ì‹œ, ì„œë¹„ìŠ¤ ë“± ê¶ê¸ˆí•œ ê²Œ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë´ ì£¼ì„¸ìš”!"
  example:  
  "ë‚˜ ë‚´ì‹  2ë“±ê¸‰ì¸ë° ê²½í¬ëŒ€ ê°ˆ ìˆ˜ ìˆì–´?"
  Expection:  
  êµê³¼ ì„±ì  í‰ê°€ ë°©ì‹(ì„±ì·¨ë„/ë“±ê¸‰ ë“±)ì„ ë¬»ëŠ” ê²½ìš°ëŠ” ì •ìƒ ë‹µë³€  
    example: 
    "ë¯¸ì ë¶„ ì„±ì  ì–´ë–»ê²Œ ë‚˜ì™€?" â†’ normal reply
    
- At the end of the answer, always add:  
  "ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! í•„ìš”ì‹œì— ì¹´í…Œê³ ë¦¬ ì¬ì„¤ì • ë²„íŠ¼ì„ í†µí•´ ì¹´í…Œê³ ë¦¬ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸ˜Š"

"""
    ),
        ("human", "Answer the following question using these documents:\n\n[Documents]\n{documents}\n\n[Question]\n{question}"),
    ])

    rag_chain = prompt_with_context | llm | StrOutputParser()
    generation = rag_chain.invoke({
        "documents": documents_text,
        "question": question                                    
    })

    # ë©”ëª¨ë¦¬ & ë¡œê·¸ ì €ì¥
    memory.chat_memory.add_user_message(HumanMessage(content=question))
    memory.chat_memory.add_ai_message(AIMessage(content=generation))

    save_chat_log(question, generation, user_id=user_id, category=category)

    return {"generation": generation}


from langchain_core.messages import HumanMessage, AIMessage
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser  

def llm_fallback_adaptive(state: AdaptiveRagState):
    question = state.get("question", "")
    user_id = state.get("user_id", "anonymous")
    category = state.get("category", "ë¯¸ì§€ì •")

    # ìœ ì €ë³„ memory ê°€ì ¸ì˜¤ê¸°
    memory = get_user_memory(user_id)

    # ì´ì „ ëŒ€í™” context êµ¬ì„±
    history = memory.chat_memory.messages
    history_text = "\n".join([
        f"User: {m.content}" if isinstance(m, HumanMessage) else f"Bot: {m.content}"
        for m in history
    ])

    # LLM Fallback í”„ë¡¬í”„íŠ¸ ì •ì˜
    prompt_with_context = ChatPromptTemplate.from_messages([
        ("system","""You are an AI assistant helping with various topics. Follow these guidelines:

    There are five possible situations:

    1. If the question is relevant to topics like school policies, curriculum, admissions, book, or services ë“± í•™êµì— ê´€ë ¨ëœ ì •ë³´,
        respond by clearly stating: "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.ğŸ˜Š"

    2. If the question is unrelated to those topics (e.g., public holidays, general culture, history, daily life),
       simply answer it using your general knowledge.
       
    3. If the question is about "ì„¸ë¶€íŠ¹ê¸° ëŠ¥ë ¥ì‚¬í•­ ì£¼ì œ/í™œë™/íƒêµ¬/ë³´ê³ ì„œ/ì‹¤í—˜ì„ ì¶”ì²œí•´ì¤˜"(excluding any kind of subject/course/book suggestions):  
        Respond with:  
        "ì„¸íŠ¹ ì¶”ì²œì€ ë§ˆì´í´ë¦¬ì˜¤ <ì„¸íŠ¹ ì¶”ì²œ> ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”!ğŸ˜Š  
        ì•„ë˜ì˜ ë§í¬ë¥¼ í´ë¦­í•˜ì‹œë©´ ì„¸íŠ¹ ì¶”ì²œ ì„œë¹„ìŠ¤ë¡œ ì´ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. >> https://myfolio.im/seteuk"
        
    4. If the Inquiry is about Myfolio services (e.g., ì„¸íŠ¹ ì¶”ì²œ, ìƒê¸°ë¶€ ë¡œë“œë§µ, ë…ì„œ ì¶”ì²œ ë“±):  
        Respond with:  
        "ì„œë¹„ìŠ¤ ë¬¸ì˜ëŠ” ì•„ë˜ ë§í¬ë¥¼ í†µí•´ ìƒë‹´ì›ê³¼ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. >> CS ë§í¬"
    
    5. If the user's input appears to be a greeting, farewell, or expression of gratitude (e.g., "ê³ ë§ˆì›Œìš”", "ê°ì‚¬í•©ë‹ˆë‹¤", "ì˜ ì“¸ê²Œìš”", "ìˆ˜ê³ í•˜ì„¸ìš”", "ì•ˆë…•", etc.),  
        respond with the following friendly message:
        "ê°ì‚¬í•©ë‹ˆë‹¤. ì…ì‹œ ê´€ë ¨ ì§ˆë¬¸ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë´ì£¼ì„¸ìš”! ğŸ˜Š"

    In all cases:
    - Provide accurate and helpful information to the best of your ability.
    - Express uncertainty when unsure; avoid speculation.
    - Keep answers concise yet informative.
    - Inform users they can ask for clarification if needed.
    - Respond ethically and constructively.
    - Mention reliable general sources when applicable if needed.
    """),
        ("human", "{question}"),
    ])

    llm_chain = prompt_with_context | llm | StrOutputParser()
    generation = llm_chain.invoke({"question": question})

    # ë©”ëª¨ë¦¬ì— ì €ì¥
    memory.chat_memory.add_user_message(HumanMessage(content=question))
    memory.chat_memory.add_ai_message(AIMessage(content=generation))

    # ë¡œê·¸ ì €ì¥
    save_chat_log(question, generation, user_id=user_id, category=category)

    return {"generation": generation}